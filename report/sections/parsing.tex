The next major step in the compilation process is processing the tokens received from the previous step and combine them into an abstract syntax tree (AST). This AST contains the semantics of the program, and by traversing the tree we will be able to emit the target language. 

The main two ways of writing a parser is by either utilizing a parser generator or by writing the parser by hand. This can be done by using a recursive decent technique. Depending on the situation the latter technique can have its merrits. It will provide a much finer grained control over what is possible to parse. The error messages can also be super precise. Essentially, writing a parser by hand the compiler author is able to parse any imaginable language. However, maintaining such a beast can quickly become a nightmare as the language grows and new syntax needs to be added. 

Which brings us to parser generators. Tools such as \textit{yacc} or \textit{Bison} generate parsers capable of parsing LR (1) and LALR (1) languages by taking as input a Context Free Grammar (CFG). By utilizing a parser generator we will gain greater flexibility maintaining the compiler and further expanding the acceptable language by modifying the CFG, which is the main advantage of going with this approach. However, the parse table generated for my language takes an enourmus 51 megabytes. 

There is a third way, which involves using both a parser generator and extending it by hand. This can provide much finergrained error messages to the user, as the compiler author is able to analyze the current state of the compiler at the point of failure, and figure out what went wrong by backtracking the call stack or by looking at the upcomming nodes.

For this project I will be using a parser generator. However, I will not be using an already available parser generator. I will be making my own, which I have called \textit{Parsley}. Parsley is capable of generating LR (1) parsers which fits the purpose of this project. The parser generator will solve two problems for us. Firstly, it will be used to generate a parser capable of accepting regular languages for the tokenizer. Secondly, it will generate a parser for the parsing step, which will, as stated earlier, combine the tokens from the tokenizer into an AST.

In order to create \textit{Parsley}, we first need to understand some of the underlying theory \textit{Parsley} is built upon. 

\section{Context Free Grammar}

Cotext Free Grammars are a formal way of describing a language. 
CFG consists of \textit{terminals} and \textit{non terminals}. Every grammar contains a start symbol, which is usually denoted \textit{S}. The start symbol is a non terminal, which means that it must contain at least one reduction rule. A reduction is what happens when a list of symbols match the given rule, and is able to be reduced into a non terminal. The symbols in the reducion rule can be non terminals or terminals. The notation for such a rule can be seen below.


\begin{equation*}
  \begin{split}
      S &\rightarrow aBBa \\
      B &\rightarrow b
  \end{split}
\end{equation*}

In the above example there are two reduction rules. One of which reduces into \textit{S} and one into \textit{B}. The arrow indicates that the left hand side of the arrow can be described as what lies on the right hand side of the arrow. Traditionally small letters indicate terminals and capital letters indicate a non terminal. From the above CFG, we see that the non terminal \textit{B} can only be expanded into a \textit{b}, or if we look at it in reverse, the terminal \textit{b} can be reduced into the non terminal \textit{B}. Thus the CFG can accept the language $\{abba\}$.

If we take a step back and look at CFGs with the context of compilers, the terminals are the tokens provided from the tokenizer, and the non terminals are the building blocks that combine the terminals into a meaningful tree representation. A rule for accepting an assignment statement could look like the following:

\begin{equation*}
  \begin{split}
      \text{\nont{ASSIGNMENT}} &\rightarrow \text{\term{id}   \term{eq}   \nont{EXPRESSION}} \\
  \end{split}
\end{equation*}

Here we can see that in order to make a node symbolizing an assignment we need three symbols. Two of the symbols are terminals, indicated by the \textit{t}, and a non terminal, indicated by \textit{n}, called \textit{EXPRESSION}. This non terminal can be anything that the language designer sees fit as an expression. Let us say that in this context free language an expression is a literal number. We would then have the following language:

\begin{equation*}
  \begin{split}
      S &\rightarrow \nont{ASSIGNMENT} \\
      \nont{ASSIGNMENT} &\rightarrow \text{\term{id}   \term{eq}   \nont{EXPRESSION}} \\
      \nont{EXPRESSION} &\rightarrow \term{literal} \\
  \end{split}
\end{equation*}

Then imagine the parser receiving the following stream of tokens:
\[[\term{id}, \term{eq}, \term{literal}]\]
The parser would scan the tokens from left to right one by one, and check against any potential matching rules. It is first when we reach the literal that the parser finds a match from literal to expression. The parser will then replace the terminal with the corresponding non terminal. We will then have the following: 
\[[\term{id}, \term{eq}, \nont{EXPRESSION}]\]
Now the the second rule matches the input, and we can reduce the input to the following:
\[[\nont{ASSIGNMENT}] \rightarrow [S]\]

We now have an idea of what we want to build. We want to build a machine that takes some CFG and converts this into another machine that accepts the language of the given CFG. This machine is the actual parser that will be used for the parsing step of the compiler.

We will now dive into the theory necessary constructing such a machine. To build the parsetable we need to understand three concepts: NULL, FIRST and FOLLOW.

\section{NULL Set}

The name NULL comes from the fact that some non terminals are nullable. Nullable symbols are non terminals that have some rules where the entire right side of the production rule can be replaced with nothing, which indicated with an $\epsilon$. Imagine the following rules:

\begin{equation*}
  \begin{split}
      S &\rightarrow ABC \\
      A &\rightarrow AC \hspace*{3px}|\hspace*{3px}B\\
      B &\rightarrow b \hspace*{3px}|\hspace*{3px}\epsilon \\
      C &\rightarrow c
  \end{split}
\end{equation*}

Looking at the third reduction rule we recognize that the non terminal \textit{B} can either be achieved by reading a \textit{b} from the input or nothing at all. This means that the non terminal \textit{B} is nullable, because it can be replaced with nothing. By transitivity this means that \textit{A} is also nullable, because \textit{A} has a reduction rule consisting of entirely nullable symbols. Thus for the above CFG we have the nullable set:
\[\SET{A, B}\]

\section{FIRST Set}

The first set is the collection of terminals that can appear as the first symbol in the reduction rule for each non terminal. Thus, the first set for the above CFG will have four collections of terminals, one for each non terminal: S, A, B and C. However, we must remember that some of the non terminals are nullable. This means that FIRST(A) contains both \textit{b} and \textit{c}. This is due to the fact that \textit{A} itself is nullable, and thus the first reduction rule for \textit{A} can be rewritten as follows:


\begin{equation*}
  \begin{split}
      A &\rightarrow C
  \end{split}
\end{equation*}

From what we know now, we can compute the first set for each non terminal:
\begin{itemize}
  \item FIRST(S) = $\SET{FIRST(A)}$ = $\SET{b, c}$
  \item FIRST(A) = $\SET{FIRST(B), FIRST(C)}$ = $\SET{b, c}$
  \item FIRST(B) = $\SET{b}$
  \item FIRST(C) = $\SET{c}$
\end{itemize}

\section{FOLLOW Set}

The follow set is the collection of symbols that may appear after a non terminal. Just like the first set it contains four collection of terminals.

\section{Constructing a Parse Table}

Using the above three sets, we can construct a parse table which will be used to parse some input given a grammar. On success this will result in an AST. The parse table is conceptually a state machine, where each arc is a symbol as defined in the grammar. However, it is not a DFA because this state machine only contains arcs that comply with the language as defined by the grammar. The parse table is constructed by running through two phases until no new states can be added. The first phase computes the closure of the given state. The second phase branches from a given state to the next state with a symbol.

The start state of this machine starts with the start symbol and it's production rule of the grammar. Using the following grammar:

\begin{figure}[H]
  \begin{equation*}
    \begin{split}
        S &\rightarrow E\$\\
        E &\rightarrow T + E\\
        E &\rightarrow T\\
        T &\rightarrow x\\
    \end{split}
  \end{equation*}
  \caption{CFG}\label{cfgparser}
\end{figure}

We will get the following:
\begin{equation*}
  \begin{split}
      S &\rightarrow [\bullet E\text{\$}, \SET{?}] 
  \end{split}
\end{equation*}

The dot indicates which symbol the parser expects to see next, if it were to reduce using that production rule. The dollar sign is an End Of Parse (EOP) symbol indicating that the parser has successfully read the entire input. The questionmark is the lookahead symbol which is the symbol the parser expects to see if this given production rule's reduction. Since nothing is supposed to come after the the start production it is marked with a questionmark indicating that nothing is expected after. 

Let us now dive into the first phase. The goal of this phase is to include all the rules from the grammar that this state can produce. For each production in the state take the symbol just right of the dot and if it is a nonterminal add the production rules of that nonterminal. This will give us the following initial state:

\begin{figure}[htp!]
\begin{equation*}
  \begin{split}
      S &\rightarrow [\bullet E\$, \SET{?}]\\
      E &\rightarrow [\bullet T+E, \SET{\$}] \\
      E &\rightarrow [\bullet T, \SET{\$}] \\
      T &\rightarrow [\bullet x, \SET{+,\$}] \\
  \end{split}
\end{equation*}
\caption*{State 1}
\end{figure}

The lookahead symbol has for the added entries changed to a dollar symbol for the second and third entry and for the last entry it contains both the plus symbol and the dollar symbol. The algorithm assigns the lookahead for each new entry by computing the FOLLOW set from the entry from which it was added. However, from the parsers perspective everything to the left of the dot in the entry has already been parsed. Therefore, from the parser's perspective, everything up until the dot is NULLABLE. The last entry contains two lookahead symbols. This is due to the fact that the T symbol has been added from entries two and three. The lookahead from entry 2 is $\SET{+}$ and the lookahead from entry 3 is $\SET{\$}$. 

Now that the closure phase has finished the second phase begins. For each entry in the state make an arc, traversable using the symbol just right of the dot, to a new state. From the old state copy each entry that contains that symbol to the right of the dot to the new state. From here advance each entry's dot by one to the right. The last step in the second phase is to goto the first phase with the new state. This will give us the following states:

\begin{figure}[H]
\makebox[\textwidth][c]{
  \subfloat[State 2]{
    \begin{minipage}{0.3\linewidth}
      \begin{equation*}
        \begin{split}
            S &\rightarrow [E\bullet \$, \{\text{?}\}]
        \end{split}
      \end{equation*}
    \end{minipage}}

  \subfloat[State 3]{
    \begin{minipage}{0.3\linewidth}
        \begin{equation*}
          \begin{split}
              E &\rightarrow [T\bullet + E, \{\$\}]\\
              E &\rightarrow [T\bullet, \{\$\}]
          \end{split}
        \end{equation*}
    \end{minipage}}

  \subfloat[State 4]{
    \begin{minipage}{0.3\linewidth}
        \begin{equation*}
          \begin{split}
            T &\rightarrow [x\bullet, \{+,\$\}]
          \end{split}
        \end{equation*}
    \end{minipage}}
}
\end{figure}

The above grammar produces a total of six states, \textit{all are not shown}. From these six states we can produce the parse table. For each state there is an action that can take place depending on the current symbol from the input. 

The parser uses a stack to keep track of symbols it has already checked.

\begin{enumerate}
  \item \textbf{Accept}: When reaching this the parser is in an accepting state which terminates the parser and returns the AST.
  \item \textbf{Shift($i$)}: This action eats a symbol from the input and moves it to the stack. Then it does a Goto($i$). 
  \item \textbf{Goto($i$)}: This action changes the parser's state to state $i$.
  \item \textbf{Reduce($k$)}: This action tells the parser that it should reduce using rule $k$. By doing this it pops $n$ states from the stack and push the left hand side of the rule to the stack, where $n$ is the number of symbols on the right hand side of the rule.
  \item \textbf{Syntax Error}: The parser stumbled across an unexpected symbol. It expected one of the symbols with any of the above four actions in the current state.
\end{enumerate}

Now that we know what each action does, we can go through the rules for deciding which action to populate the parse table with. For each state look through each entry and decide which one of the four possible conditions are met:
\begin{enumerate}
  \item If the symbol to the right of the dot is a dollar, put an accept action.
  \item If the symbol to the right of the dot is a terminal, put a shift action and let $i$ be the id of the state reachable by traversing using that symbol.
  \item If the symbol to the right of the dot is a non terminal, put a hoto action and let $i$ be the id of the state reachable by traversing using that symbol.
  \item If no symbols are to the right of the dot, add a reduce action for each symbol in the lookahead set with the $k$'th rule.
\end{enumerate}

By following the above directions, we can produce a parse table for the above given grammar:

\begin{table}[H]
  \centering
  \begin{tabular}{|l|lll|ll|}
  \hline
  State/Symbol & x  & +  & \$ & E  & T  \\ \hline
  1            & s4 &    &    & g2 & g3 \\
  2            &    &    & a  &    &    \\
  3            &    & s5 & r2 &    &    \\
  4            & s4 &    &    & g6 & g3 \\
  5            &    & r3 & r3 &    &    \\
  6            &    &    & r1 &    &    \\ \hline
  \end{tabular}
  \caption*{Parse Table constructed from CFG~\ref{cfgparser}}
\end{table}
If at any point the parser sees a symbol that has an empty cell, it will produce a syntax error.

\section{Parsley}

Parsley is the finished parser generator capable of producing an LR(1) parser given any CFG. Using Parsley we can construct a parser for the tokenizer capable of parsing Regex. Using the parser from Parsley we provide that parser the syntax of the language we want and that will return a DFA capable of tokenizing the source code. With this the first step of the frontend is finished. The second step of the frontend is the parser for the actual languange syntax. Just like with constructing a parser capable of recognizing Regex, we provide Parsley with the CFG for the language and it returns a parser for the compiler. These steps of the compiler chain are only present while  developing the compiler. The diagram below shows how the frontend fits together
\begin{figure}[H]
\begin{tikzpicture}[node distance=4cm]
  \node (parsley) [square] {Parsley};
  \node (cfg) [bluebox, above of=parsley, yshift=-10mm] {Language Definitions};
  \node (regexparser) [square, right of=parsley, xshift=10mm] {Regex Parser};
  \node (syntax) [bluebox, above of=regexparser, yshift=-10mm] {Syntax Definitions};
  \node (tokenizer) [parserstyle, right of=regexparser, xshift=10mm] {Tokenizer};
  \node (sourcecode) [bluebox, above of=tokenizer, yshift=-10mm] {Source Code};
  \node (parser) [parserstyle, below of=tokenizer, yshift=10mm] {Parser};

  \draw [arrow] (parsley) -- node[anchor=south] {Parser} (regexparser);
  \draw [arrow] (parsley) |- node[xshift=25mm, anchor=south west] {Parser} (parser);
  \draw [arrow] (regexparser) -- node[anchor=south] {DFA}(tokenizer);
  \draw [arrow] (cfg) -- node[anchor=west] {CFG} (parsley);
  \draw [arrow] (syntax) -- node[anchor=west] {Regex} (regexparser);
  \draw [arrow] (sourcecode) -- node[anchor=east] {Plain Text} (tokenizer);
  \draw [arrow] (tokenizer) -- node[anchor=east] {List of Tokens} (parser);
\end{tikzpicture}
\caption{Illustration of the Frontend for the Compiler}
\end{figure}

In the above figure the first two blue boxes are modifiable by the compiler author. Language Definitions define the language's syntax. Syntax Definitions define the syntax for keywords, legal variable names, etc. The Source Code box is the source code the user wants to compile. The red boxes are intermediary programs that are used for constructing the compiler. The orange boxes are part of the compiler and are used in production.
